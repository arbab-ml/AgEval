{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded 20 samples from /Users/muhammadarbabarshad/Downloads/AgEval-datasets/FUSARIUM-22/dataset_raw\n",
      "Running model: GPT-4o \n",
      " over dataset: FUSARIUM 22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 318\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 318\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/miniconda3/envs/gpt4o-env1/lib/python3.9/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt4o-env1/lib/python3.9/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt4o-env1/lib/python3.9/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt4o-env1/lib/python3.9/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_vendors_models=[\n",
    "    {\"vendor\": \"openai\", \"model\": \"gpt-4o-2024-05-13\", \"model_name\": \"GPT-4o\"},\n",
    "    {\"vendor\": \"anthropic\", \"model\": \"claude-3-sonnet-20240229\", \"model_name\": \"Claude-3-sonnet\"},\n",
    "    {\"vendor\": \"openrouter\", \"model\": \"google/gemini-flash-1.5\", \"model_name\": \"Gemini-flash-1.5\"},\n",
    "    {\"vendor\": \"openrouter\", \"model\": \"liuhaotian/llava-yi-34b\", \"model_name\": \"LLaVA v1.6 34B\"}, \n",
    "]\n",
    "# load all the modules local again instead of cache\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import the required libraries\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from anthropic import Anthropic\n",
    "from PIL import Image\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import nest_asyncio\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from data_loader import load_and_prepare_data_SBRD, load_and_prepare_data_DurumWheat, load_and_prepare_data_soybean_seeds, load_and_prepare_data_mango_leaf, load_and_prepare_data_DeepWeeds, load_and_prepare_data_IP02, load_and_prepare_data_bean_leaf, load_and_prepare_data_YellowRust, load_and_prepare_data_FUSARIUM22\n",
    "nest_asyncio.apply()\n",
    "global vision_prompt\n",
    "vision_prompt = \"\"\n",
    "def extract_json(s):\n",
    "    \"\"\"Extract the first JSON object from a string.\"\"\"\n",
    "    json_match = re.search(r'\\{.*\\}', s, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def load_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load image from file, convert to JPEG, and encode as base64.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format=\"JPEG\", quality=95)\n",
    "            return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_requests, time_window):\n",
    "        self.max_requests = max_requests\n",
    "        self.time_window = time_window\n",
    "        self.request_times = []\n",
    "\n",
    "    async def wait(self):\n",
    "        while True:\n",
    "            current_time = time.time()\n",
    "            self.request_times = [t for t in self.request_times if t > current_time - self.time_window]\n",
    "            if len(self.request_times) < self.max_requests:\n",
    "                self.request_times.append(current_time)\n",
    "                break\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "class GPTAPI:\n",
    "    def __init__(self, api_key, model):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        self.rate_limiter = RateLimiter(max_requests=10, time_window=10)\n",
    "\n",
    "    async def get_image_information(self, inputs: dict) -> str:\n",
    "        await self.rate_limiter.wait()\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": inputs['prompt']},\n",
    "                        *inputs['examples'],\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{inputs['image']}\",\n",
    "                                \"detail\": \"high\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 4096, \n",
    "            \"temperature\":1.0\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(self.url, headers=self.headers, json=payload) as response:\n",
    "                result = await response.json()\n",
    "                if \"choices\" in result and result[\"choices\"]:\n",
    "                    return result[\"choices\"][0]['message']['content']\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected API response format: {result}\")\n",
    "\n",
    "class ClaudeAPI:\n",
    "    def __init__(self, api_key, model):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.rate_limiter = RateLimiter(max_requests=5, time_window=60)  # Adjust these values as needed\n",
    "\n",
    "    async def get_image_information(self, inputs: dict) -> str:\n",
    "        await self.rate_limiter.wait()\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": inputs['prompt']},\n",
    "                    *[\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/jpeg\",\n",
    "                                \"data\": ex['image_url']['url'].split(',')[1] if ex['type'] == 'image_url' else ex['source']['data']\n",
    "                            }\n",
    "                        }\n",
    "                        if ex['type'] in ['image_url', 'image'] else ex\n",
    "                        for ex in inputs['examples']\n",
    "                    ],\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": inputs['image']\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=4096,\n",
    "            temperature=1.0,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.content[0].text\n",
    "# Set the base directory\n",
    "\n",
    "\n",
    "class OpenRouterAPI:\n",
    "    def __init__(self, api_key, model):#  liuhaotian/llava-yi-34b\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "        }\n",
    "        self.rate_limiter = RateLimiter(max_requests=5, time_window=10)  # Adjust as needed\n",
    "\n",
    "    async def get_image_information(self, inputs: dict) -> str:\n",
    "        await self.rate_limiter.wait()\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": inputs['prompt']},\n",
    "                        *inputs['examples'],\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{inputs['image']}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\":1.0\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(self.url, headers=self.headers, json=payload) as response:\n",
    "                result = await response.json()\n",
    "                if \"choices\" in result and result[\"choices\"]:\n",
    "                    return result[\"choices\"][0]['message']['content']\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected API response format: {result}\")\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, total):\n",
    "        self.pbar = tqdm(total=total, desc=\"Processing images\")\n",
    "\n",
    "    def update(self):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "################################################################################################################################################################\n",
    "total_samples_to_check = 20\n",
    "vendor, model, model_name =all_vendors_models[0].values() # index 2 is gemini, 3 is llava\n",
    "\n",
    "async def process_image(api, i, number_of_shots, all_data_results, all_data, progress_bar):\n",
    "    try:\n",
    "        image_path = all_data[0][i]\n",
    "        image_base64 = load_image(image_path)\n",
    "        if image_base64 is None:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "        \n",
    "        examples = []\n",
    "        example_paths = []\n",
    "        example_categories = []\n",
    "        num_rows = len(all_data)\n",
    "        random_indices = random.sample([idx for idx in range(num_rows) if idx != i], number_of_shots)\n",
    "\n",
    "        for j in random_indices:\n",
    "            example_image_path = all_data[0][j]\n",
    "            example_image_base64 = load_image(example_image_path)\n",
    "            if example_image_base64 is not None:\n",
    "                if isinstance(api, GPTAPI) or isinstance(api, OpenRouterAPI):\n",
    "                    examples.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{example_image_base64}\", \"detail\": \"high\"}})\n",
    "                elif isinstance(api, ClaudeAPI):\n",
    "                    examples.append({\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": example_image_base64\n",
    "                        }\n",
    "                    })\n",
    "                examples.append({\"type\": \"text\", \"text\": f'{{\"prediction\": \"{all_data.at[j, 1]}\"}}' })\n",
    "                example_paths.append(example_image_path)\n",
    "                example_categories.append(all_data.at[j, 1])\n",
    "\n",
    "        prediction = await api.get_image_information({\"image\": image_base64, \"examples\": examples, \"prompt\": vision_prompt})\n",
    "        \n",
    "        try:\n",
    "            extracted_json = extract_json(prediction)\n",
    "            parsed_prediction = extracted_json['prediction']\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON for image {image_path}. API response: {prediction}. Error: {str(e)}\")\n",
    "            parsed_prediction = 'NA'\n",
    "\n",
    "\n",
    "        all_data_results.at[i, f\"# of Shots {number_of_shots}\"] = parsed_prediction\n",
    "        all_data_results.at[i, f\"Example Paths {number_of_shots}\"] = json.dumps(example_paths)\n",
    "        all_data_results.at[i, f\"Example Categories {number_of_shots}\"] = json.dumps(example_categories)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {all_data[0][i]}: {str(e)}\")\n",
    "        all_data_results.at[i, f\"# of Shots {number_of_shots}\"] = 'NA'\n",
    "        all_data_results.at[i, f\"Example Paths {number_of_shots}\"] = 'NA'\n",
    "        all_data_results.at[i, f\"Example Categories {number_of_shots}\"] = 'NA'\n",
    "    finally:\n",
    "        progress_bar.update()\n",
    "\n",
    "async def process_images_for_shots(api, number_of_shots, all_data_results, all_data):\n",
    "    progress_bar = ProgressBar(len(all_data))\n",
    "    tasks = []\n",
    "    for i in range(len(all_data)):\n",
    "        task = asyncio.ensure_future(process_image(api, i, number_of_shots, all_data_results, all_data, progress_bar))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    await asyncio.gather(*tasks)\n",
    "    progress_bar.close()\n",
    "\n",
    "async def main():\n",
    "    all_data, expected_classes,output_file_name = load_and_prepare_data_FUSARIUM22( total_samples_to_check)\n",
    "    global vision_prompt\n",
    "    # vision_prompt = f\"\"\"\n",
    "    # Given the image, identify the class. Use the following list of possible classes for your prediction It should be one of the : {expected_classes}. Be attentive to subtle details as some classes may appear similar. Provide your answer in the following JSON format:\n",
    "    # {{\"prediction\": \"class_name\"}}\n",
    "    # Replace \"class_name\" with the appropriate class from the list above based on your analysis of the image.\n",
    "    # The labels should be entered exactly as they are in the list above i.e., {expected_classes}.\n",
    "    # The response should start with {{ and contain only a JSON object (as specified above) and no other text.\n",
    "    # \"\"\"\n",
    "\n",
    "    vision_prompt = f\"\"\"\n",
    "    Given the image, identify the class. Use the following list of possible classes for your prediction It should be one of the : {expected_classes}. Be attentive to subtle details as some classes may appear similar. Provide your answer in the following JSON format:\n",
    "    {{\"prediction\": \"class_name\"}}\n",
    "    Replace \"class_name\" with the appropriate class from the list above based on your analysis of the image.\n",
    "    The labels should be entered exactly as they are in the list above i.e., {expected_classes}.\n",
    "    The response should start with {{ and contain only a JSON object (as specified above) and no other text.\n",
    "    \"\"\"\n",
    "    all_data_results = all_data.copy(deep=True)\n",
    "    all_data_results.columns = all_data_results.columns.map(str)\n",
    "\n",
    "    if vendor == \"openai\":\n",
    "        api = GPTAPI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=model)\n",
    "    elif vendor == \"anthropic\":\n",
    "        api = ClaudeAPI(api_key=os.getenv(\"ANTHROPIC_API_KEY\"), model=model)\n",
    "    elif vendor == \"openrouter\":\n",
    "        api = OpenRouterAPI(api_key=os.getenv(\"OPENROUTER_API_KEY\"), model=model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {vendor}\")\n",
    "\n",
    "    # print we are running model model_name and output_file_name\n",
    "    print(f\"Running model: {model_name} \\nover dataset: {output_file_name}.csv\")\n",
    "\n",
    "    # \n",
    "    # \n",
    "    #  \n",
    "    for number_of_shots in [1]:  # [0, 1, 2, 4, 8]:\n",
    "        await process_images_for_shots(api, number_of_shots, all_data_results, all_data)\n",
    "    \n",
    "    # Create the results directory structure\n",
    "    results_dir = os.path.join(\"results\", model_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the results file\n",
    "    output_file = os.path.join(results_dir, f\"{output_file_name}.csv\")\n",
    "    all_data_results.to_csv(output_file)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt4o-env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
