{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "We evaluated the vision capabilities of the GPT-4o language model (identifier gpt-4o-2024-05-13) using a series of tasks that assess its performance on visual understanding and reasoning. For each task, a dataset of approximately 100 representative images was curated.\n",
    "\n",
    "The model was provided an image along with a text prompt specifying the desired output format. The prompts were designed to probe the model's ability to identify, classify, describe, and analyze visual content without additional context. For select tasks, we further investigated the model's few-shot learning capabilities by providing a small number of labeled examples before the query image.\n",
    "\n",
    "Model outputs were compared against ground truth labels to compute standard performance metrics such as accuracy. Qualitative analysis was also conducted on a subset of responses to identify common failure modes and strengths. The results across different tasks provide insights into GPT-4o's current visual understanding capabilities, areas for improvement, and potential as a foundation model for vision tasks.\n",
    "\n",
    "Subsequent sections delve into the specifics of each task, dataset, and findings, offering a comprehensive evaluation of GPT-4o's visual reasoning skills and informing future research directions for large language models applied to multimodal tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fruits Images Dataset consists of approximately 400 images spanning 10 different fruit classes: Banana, Jackfruit, Mango, Litchi, Hog Plum, Papaya, Grapes, Apple, Orange, and Guava. Each fruit class has 40 labeled images, with the dataset split into 320 training images and 80 test images. The images were collected from various internet sources such as Google Images and stock image websites, and were labeled by the dataset creators. For this evaluation, the model was provided an image along with a prompt to identify the fruit class from the list of 10 classes in a specified format. Model predictions were compared against ground truth labels to assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"01-Fruit-high-setting.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions = data['# of Shots 0']\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=pd.unique(data['1']))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pd.unique(data['1']))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"01-Fruit.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract ground truth and predictions\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions = data['# of Shots 0']\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predictions, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "index = np.arange(len(pd.unique(data['1'])))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(pd.unique(data['1']), rotation=90)\n",
    "ax.set_ylim(0.8, 1)  # Adjust y-axis\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"01-Fruit.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions = data['# of Shots 0']\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predictions, average=None)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "result_df = pd.DataFrame({\n",
    "    'Class': pd.unique(data['1']),\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model demonstrated strong performance in classifying the 10 different fruit classes, achieving high precision, recall, and F1-scores across most classes. Several classes, including Papaya, Apple, Litchi, Hog Plum, Grapes, and Guava, obtained perfect scores of 1.0 for precision, recall, and F1-score. The Banana and Mango classes had slightly lower but still impressive precision scores of 0.909091, with perfect recall of 1.0. Jackfruit and Orange classes had precision scores of 1.0, but slightly lower recall scores of 0.9, resulting in F1-scores of 0.947368. Overall, the model exhibited remarkable accuracy in recognizing and distinguishing the visual characteristics of these fruit categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drowsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Drowsy Detection Dataset consists of images extracted from videos capturing drivers in three distinct states: natural, fatigued, and drowsy. The dataset was curated by gathering relevant videos, converting them into image frames, and applying facial detection algorithms to isolate key facial regions like eyes, mouth, and cheeks, which are indicative of drowsiness. The extracted images were converted to grayscale, resized to 48x48 pixels, and accurately labeled based on the driver's state. The dataset comprises two classes: 'DROWSY' and 'NATURAL', with a total of 100 labeled images sampled evenly from each class. For this evaluation, GPT-4o was provided an image along with a prompt to classify it into one of the two classes in a specified JSON format. The model's predictions were compared against the ground truth labels to assess its performance in detecting driver drowsiness from facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"02-Drowsy.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions = data['# of Shots 0']\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=pd.unique(data['1']))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pd.unique(data['1']))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"02-Drowsy.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract ground truth and predictions\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions = data['# of Shots 0']\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, predictions, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "index = np.arange(len(pd.unique(data['1'])))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(pd.unique(data['1']), rotation=90)\n",
    "ax.set_ylim(0.6, 1)  # Adjust y-axis\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Class': pd.unique(data['1']),\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Crop Disease Classification Dataset is a comprehensive collection of images aimed at evaluating GPT-4o's capabilities in identifying crop diseases. The dataset encompasses 20 distinct classes of common crop diseases: Blight, Downy Mildew, Apple Scab, Cedar Apple Rust, Blossom End Rot, Fire Blight, Verticillium, Fusarium, Canker, Gray Mold, Botrytis, Powdery Mildew, Nematodes, Black Spot, Leaf Spots, Anthracnose, Mosaic Virus, Brown Rot, Crown Gall, and Clubroot. For this evaluation, a total of 100 images were sampled from the dataset, with each class represented by approximately 5 images. GPT-4o was provided these images along with a prompt to classify the crop disease depicted in each image. The model's predictions were compared against the ground truth labels to assess its performance in accurately identifying and distinguishing various crop diseases based solely on visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path =\"03-Crop.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions = data['# of Shots 0'].astype('str')\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=pd.unique(data['1']))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pd.unique(data['1']))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"03-Crop.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 0'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    " \n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print lengths of precision, recall and f1\n",
    "print(len(precision), len(recall), len(f1), len(classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Glaucoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Glaucoma Detection Dataset used for this evaluation consisted of retinal fundus images from the ACRIMA database. A subset of 100 images was sampled, evenly split between glaucomatous and normal cases. These images were collected at FISABIO Oftalmolog√≠a M√©dica in Valencia, Spain, from patients with prior consent, and were annotated by experienced glaucoma experts. GPT-4o was tasked with classifying each image into either the \"glaucoma\" or \"normal\" category based solely on the visual information provided. The model's predictions were compared against the expert-annotated ground truth labels to assess its performance in detecting glaucoma from retinal fundus imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data_path = \"04-Glaucoma.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions = data['# of Shots 4'].astype('str')\n",
    "\n",
    "# Print the unique values and their indices for ground_truth and predictions\n",
    "print(\"Ground Truth:\")\n",
    "print(pd.unique(ground_truth))\n",
    "print(ground_truth.index)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(pd.unique(predictions))\n",
    "print(predictions.index)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=pd.unique(data['1']))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pd.unique(data['1']))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"04-Glaucoma.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 4'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that previously the values for maximum number of shots (number of examples fed to complete the task) is provided. we can look at how the evolution happened with respect to the number of examples given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"04-Glaucoma.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 4]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 4], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 4])\n",
    "ax.set_xticklabels([f'# of Shots {i}' for i in [0, 1, 4]])\n",
    "ax.set_ylim(0.00, 1)  # Adjust y-axis\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 4]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Computed Tomography (CT) Brain Scan dataset contains CT images of the brain aimed at detecting and classifying various conditions such as cancer, tumors, and aneurysms. For this evaluation, a subset of 100 CT scan images was sampled from the dataset. GPT-4o was tasked with analyzing these images and classifying them into one of three categories: \"cancer\", \"tumor\", or \"aneurysm\". The model's predictions were compared against the ground truth labels to assess its performance in identifying these medical conditions from CT brain imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data_path = \"05-CT.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions = data['# of Shots 0'].astype('str')\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(ground_truth, predictions, labels=pd.unique(data['1']))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pd.unique(data['1']))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Convert the confusion matrix to a dataframe\n",
    "cm_df = pd.DataFrame(cm, index=pd.unique(data['1']), columns=pd.unique(data['1']))\n",
    "\n",
    "# Print the confusion matrix dataframe\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix reveals that the model completely failed to predict the 'cancer' class, potentially due to lack of representative training data or inherent similarities with other classes. Additionally, it struggled to distinguish between 'aneurysm' and 'tumor' classes, with significant misclassifications in both directions, suggesting a need for further fine-tuning or incorporation of additional relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 captions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Flickr8k-Images-Captions dataset is a compact collection designed for image captioning tasks. It consists of 8,000 images sourced from Flickr, each accompanied by multiple human-annotated captions describing the visual content. For this evaluation, a subset of 100 images from the dataset was used. GPT-4o was tasked with inferring the context and generating natural language descriptions for these 100 images. The model's generated captions were compared against the ground truth human captions using the BLEU score, a metric that measures the similarity between machine and human-generated texts. This dataset provided a testbed for assessing GPT-4o's ability to comprehend visual scenes and translate them into accurate and coherent textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Read the CSV file\n",
    "data = []\n",
    "with open('06-Captions.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Extract the ground truth and predicted captions\n",
    "ground_truth = [row[2] for row in data]\n",
    "predicted = [row[3] for row in data]\n",
    "\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_1 = corpus_bleu([[gt.split()] for gt in ground_truth], [pred.split() for pred in predicted], weights=(1, 0, 0, 0))\n",
    "bleu_2 = corpus_bleu([[gt.split()] for gt in ground_truth], [pred.split() for pred in predicted], weights=(0.5, 0.5, 0, 0))\n",
    "bleu_3 = corpus_bleu([[gt.split()] for gt in ground_truth], [pred.split() for pred in predicted], weights=(0.33, 0.33, 0.33, 0))\n",
    "bleu_4 = corpus_bleu([[gt.split()] for gt in ground_truth], [pred.split() for pred in predicted], weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "\n",
    "print(f\"BLEU-1: {bleu_1:.3f}\")\n",
    "print(f\"BLEU-2: {bleu_2:.3f}\")\n",
    "print(f\"BLEU-3: {bleu_3:.3f}\")\n",
    "print(f\"BLEU-4: {bleu_4:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Visual Question Answering (VQA) dataset is a multimodal benchmark that combines computer vision and natural language processing tasks. It consists of images paired with natural language questions related to the visual content. The goal is to produce accurate natural language answers by comprehending the semantics of both the image and the question. For this evaluation, a subset of 100 image-question pairs was sampled from the dataset. GPT-4o was tasked with analyzing the provided image and the corresponding question, and generating an appropriate answer chosen from a predefined list of possible answers. The model's generated answers were compared against the ground truth answers to assess its performance in this AI-complete task, which involves a wide range of sub-problems such as object detection, scene classification, and multimodal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"07-VQA-all.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['answer'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "#calculate overall accuracy\n",
    "accuracy = (ground_truth == predictions).mean()\n",
    "print(f\"Overall accuracy: {accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now checking how the accuracy evolved after giving it few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"07-VQA-all.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Define the shot columns\n",
    "shot_columns = ['# of Shots 0', '# of Shots 1', '# of Shots 2', '# of Shots 4', '# of Shots 8']\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['answer'].astype(str)\n",
    "\n",
    "# Calculate accuracy for each shot\n",
    "accuracies = []\n",
    "for shot_column in shot_columns:\n",
    "    predictions = data[shot_column].astype(str)\n",
    "    accuracy = (ground_truth == predictions).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = np.arange(len(shot_columns))\n",
    "width = 0.6\n",
    "ax.bar(x, accuracies, width, color='skyblue')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy for Different Numbers of Shots')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(shot_columns)\n",
    "ax.set_ylim(0.2, 0.4)\n",
    "\n",
    "# Add accuracy values on top of each bar\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    ax.text(i, accuracy + 0.01, f\"{accuracy:.3f}\", ha='center', fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the accuracies\n",
    "accuracy_df = pd.DataFrame({'Shot': shot_columns, 'Accuracy': accuracies})\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, giving few examples in a task with a lot of options does not prove very useful. On contrary, it decreases the performance if there's just one example because the distribution of answers might become skewed by this unrelated task , in case of VQA (since there are a lot of possibilities for the nature of a task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Soybeabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"results/GPT-4o/Soybean Seeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"analysis/soybean/0.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"Soybean Seeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.savefig(\"analysis/soybean/1.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"Soybean Seeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 4, 8]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 4, 8], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 4, 8])\n",
    "ax.set_xticklabels([f'{i}' for i in [0, 1, 4, 8]])\n",
    "ax.set_ylim(0.00, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"analysis/easy.png\")\n",
    "plt.savefig(\"analysis/soybean/2.png\")\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 4, 8]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Mangoü•≠ LeafüçÉüçÇ Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: MangoLeafBD Dataset\n",
    "\n",
    "Year of Availability: 2022\n",
    "\n",
    "Short Description: The MangoLeafBD dataset contains 4000 images of mango leaves, including healthy leaves and those affected by seven different diseases. Importance: This dataset is crucial for developing automated disease detection systems in mango cultivation, focusing on the classification of various leaf diseases. We specifically use the subset containing video frame pictures of durum wheat kernels for our analysis.\n",
    "\n",
    "Classes/expected output labels of dataset: \n",
    "1. Healthy\n",
    "2. Anthracnose\n",
    "3. Bacterial Canker\n",
    "4. Cutting Weevil\n",
    "5. Die Back\n",
    "6. Gall Midge\n",
    "7. Powdery Mildew\n",
    "8. Sooty Mould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"Mango Leaf Disease.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"Mango Leaf Disease/LLaVA v1.6 34B.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"Mango Leaf Disease/LLaVA v1.6 34B.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 2, 4, 8]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 2, 4, 8], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 2, 4, 8])\n",
    "ax.set_xticklabels([f'{i}' for i in [0, 1, 2, 4, 8]])\n",
    "ax.set_ylim(0.00, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"analysis/challenging_I.png\")\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 2, 4, 8]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Durum Wheat Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Durum Wheat Dataset\n",
    "Year of Availability: 2019\n",
    "\n",
    "Short Description: This dataset focuses on durum wheat kernels, specifically their vitreousness, which is crucial for producing quality food products. Importance: The dataset is significant for developing real-time sorting systems in agriculture, particularly for identifying vitreous durum wheat kernels using artificial neural networks (ANNs). We specifically use the second dataset, which contains video frame pictures of durum wheat kernels.\n",
    "\n",
    "Classes/expected output labels of dataset:\n",
    "\n",
    "Vitreous Durum Wheat Kernels\n",
    "\n",
    "Starchy Durum Wheat Kernels\n",
    "\n",
    "Foreign Matters (impurities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"10-Durum.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"10-Durum.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"10-Durum.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 4, 8]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 4, 8], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 4, 8])\n",
    "ax.set_xticklabels([f'{i}' for i in [0, 1, 4, 8]])\n",
    "ax.set_ylim(-0.05, 1.05)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 4, 8]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Bean Leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Bean Leaf Lesions Classification Dataset\n",
    "\n",
    "Year of Availability: Not specified in the given information\n",
    "\n",
    "Short Description: This dataset comprises images of bean leaf lesions in three states. Importance: It is crucial for plant pathology research and automated disease detection in agriculture. We use 100 class-balanced images from this dataset, focusing on the classification of leaf health conditions in beans.\n",
    "\n",
    "Classes/expected output labels of dataset: \n",
    "1. Healthy\n",
    "2. Angular leaf spot\n",
    "3. Bean rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"11-Bean-Leaf.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"11-Bean-Leaf.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"11-Bean-Leaf.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 2, 4, 8]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 2, 4, 8], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 2, 4, 8])\n",
    "ax.set_xticklabels([f'{i}' for i in [0, 1, 2, 4, 8]])\n",
    "ax.set_ylim(-0.05, 1.05)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 2, 4, 8]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DeepWeeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Name: DeepWeeds\n",
    "\n",
    "Year of Availability: 2019\n",
    "\n",
    "Short Description: DeepWeeds is a multiclass weed species image dataset for deep learning, containing 17,509 unique 256x256 color images across 9 classes. Importance: This dataset is crucial for developing automated weed detection systems in rangeland environments, focusing on 100 class-balanced images used for academic research in weed classification.\n",
    "\n",
    "Classes/expected output labels of dataset:\n",
    "Chinee Apple\n",
    "\n",
    "Lantana\n",
    "\n",
    "Parkinsonia\n",
    "\n",
    "Parthenium\n",
    "\n",
    "Prickly Acacia\n",
    "\n",
    "Rubber Vine\n",
    "\n",
    "Siam Weed\n",
    "\n",
    "Snake Weed\n",
    "\n",
    "Other (Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: CHANGE 'label' TO 1 in csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"12-deepweeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"12-deepweeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype(str)\n",
    "predictions = data['# of Shots 8'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Map predictions to their corresponding class indices\n",
    "mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "\n",
    "# Get the indices where mapped_predictions is not equal to -1\n",
    "valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "\n",
    "# Remove predictions that are not in the class map\n",
    "mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "ground_truth = ground_truth[valid_indices]\n",
    "\n",
    "# Create a reverse dictionary to map indices back to class labels\n",
    "reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "\n",
    "# Convert mapped_predictions back to class labels\n",
    "mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, mapped_predictions_labels, labels=classes, average=None)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(classes))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Bar plots\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "ax.bar(index + bar_width, recall, bar_width, label='Recall')\n",
    "ax.bar(index + 2*bar_width, f1, bar_width, label='F1-score')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_title('Precision, Recall, and F1 Score by Class')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "ax.set_ylim(0.2, 1)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8)  # Adjust the right spacing to accommodate the legend\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with the metrics\n",
    "metrics_df = pd.DataFrame({'Class': classes, 'Precision': precision, 'Recall': recall, 'F1-score': f1})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"12-deepweeds.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth column\n",
    "ground_truth = data['1'].astype(str)\n",
    "\n",
    "# Get the unique classes present in the ground truth\n",
    "classes = np.unique(ground_truth)\n",
    "\n",
    "# Create a dictionary to map ground truth classes to their indices\n",
    "class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "# Initialize a list to store F1 scores for each \"# of Shots\" column\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate over the \"# of Shots\" columns\n",
    "for i in [0, 1, 2, 4, 8]:\n",
    "    # Extract the prediction column\n",
    "    predictions = data[f'# of Shots {i}'].astype(str)\n",
    "    \n",
    "    # Map predictions to their corresponding class indices\n",
    "    mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "    \n",
    "    # Get the indices where mapped_predictions is not equal to -1\n",
    "    valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "    \n",
    "    # Remove predictions that are not in the class map\n",
    "    mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "    ground_truth_filtered = ground_truth[valid_indices]\n",
    "    \n",
    "    # Create a reverse dictionary to map indices back to class labels\n",
    "    reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "    \n",
    "    # Convert mapped_predictions back to class labels\n",
    "    mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average=None)\n",
    "    \n",
    "    # Append the F1 scores to the list\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Convert the list of F1 scores to a NumPy array\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot F1 scores for each class over different \"# of Shots\" columns\n",
    "for i, cls in enumerate(classes):\n",
    "    ax.plot([0, 1, 2, 4, 8], f1_scores[:, i], marker='o', label=cls)\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('# of Shots')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score over \"# of Shots\" Columns by Class')\n",
    "ax.set_xticks([0, 1, 2, 4, 8])\n",
    "ax.set_xticklabels([f'{i}' for i in [0, 1, 2, 4, 8]])\n",
    "ax.set_ylim(-0.05, 1.05)  # Adjust y-axis\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now to get the printing\n",
    "column_names = [f'# of Shots {i}' for i in [0, 1, 2, 4, 8]]\n",
    "\n",
    "# Convert the F1 scores to a dictionary with class labels as keys\n",
    "f1_scores_dict = {cls: scores for cls, scores in zip(classes, f1_scores.T)}\n",
    "\n",
    "# Create the DataFrame\n",
    "f1_scores_df = pd.DataFrame.from_dict(f1_scores_dict, orient='index', columns=column_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(f1_scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ip102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# Load the data\n",
    "data_path = \"ip02-dataset-results.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 1. Overall Accuracy Across Shots\n",
    "accuracies = []\n",
    "shot_numbers = [0, 1, 2, 4, 8]\n",
    "\n",
    "for shots in shot_numbers:\n",
    "    accuracies.append(accuracy_score(data['1'], data[f'# of Shots {shots}']))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(shot_numbers, accuracies, marker='o')\n",
    "plt.title('Overall Accuracy vs Number of Shots')\n",
    "plt.xlabel('Number of Shots')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(shot_numbers)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Top N Classes Performance\n",
    "N = 10  # Number of top classes to consider\n",
    "top_classes = data['1'].value_counts().nlargest(N).index\n",
    "\n",
    "# Calculate F1 score for each class and number of shots\n",
    "f1_scores = {}\n",
    "for shots in shot_numbers:\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        data['1'], data[f'# of Shots {shots}'], labels=top_classes, average=None\n",
    "    )\n",
    "    f1_scores[shots] = dict(zip(top_classes, f1))\n",
    "\n",
    "# Plot F1 scores for top N classes\n",
    "plt.figure(figsize=(12, 6))\n",
    "for cls in top_classes:\n",
    "    plt.plot(shot_numbers, [f1_scores[shots][cls] for shots in shot_numbers], label=cls, marker='o')\n",
    "\n",
    "plt.title(f'F1 Score for Top {N} Classes vs Number of Shots')\n",
    "plt.xlabel('Number of Shots')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(shot_numbers)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Confusion Matrix for Top N Classes (using 8-shot results)\n",
    "y_true = data['1']\n",
    "y_pred = data['# of Shots 8']\n",
    "\n",
    "# Filter for top N classes\n",
    "mask = y_true.isin(top_classes) & y_pred.isin(top_classes)\n",
    "y_true_filtered = y_true[mask]\n",
    "y_pred_filtered = y_pred[mask]\n",
    "\n",
    "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=top_classes, yticklabels=top_classes)\n",
    "plt.title(f'Confusion Matrix for Top {N} Classes (8 Shots)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Summary Statistics\n",
    "summary_stats = []\n",
    "for shots in shot_numbers:\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        data['1'], data[f'# of Shots {shots}'], average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(data['1'], data[f'# of Shots {shots}'])\n",
    "    summary_stats.append({\n",
    "        'Shots': shots,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"Summary Statistics:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Additional: Class Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "data['1'].value_counts().nlargest(N).plot(kind='bar')\n",
    "plt.title(f'Distribution of Top {N} Classes')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yellow Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: CHANGE 'label' TO 1 in csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"YELLOW-RUST-19.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and its columns to verify structure\n",
    "data.head(), data.columns\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1'].astype('str')\n",
    "predictions_0 = data['# of Shots 0'].astype('str')\n",
    "predictions_1 = data['# of Shots 8'].astype('str')\n",
    "\n",
    "# Generate the confusion matrices\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=pd.unique(data['1']))\n",
    "cm_1 = confusion_matrix(ground_truth, predictions_1, labels=pd.unique(data['1']))\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=pd.unique(data['1']))\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title('Confusion Matrix (# of Shots 0)')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_1 = ConfusionMatrixDisplay(confusion_matrix=cm_1, display_labels=pd.unique(data['1']))\n",
    "disp_1.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title('Confusion Matrix (# of Shots 8)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the data\n",
    "data_path = \"YELLOW-RUST-19.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract the ground truth and prediction columns\n",
    "ground_truth = data['1']\n",
    "predictions_0 = data['# of Shots 0']\n",
    "predictions_8 = data['# of Shots 8']\n",
    "\n",
    "# Define the order of severity for the labels\n",
    "severity_order = [\n",
    "    'No disease (0)',\n",
    "    'Resistant (R)',\n",
    "    'Moderately Resistant (MR)',\n",
    "    'MRMS',\n",
    "    'Moderately Susceptible (MS)',\n",
    "    'Susceptible (S)'\n",
    "]\n",
    "\n",
    "# Create a mapping from labels to severity indices\n",
    "label_to_index = {label: index for index, label in enumerate(severity_order)}\n",
    "\n",
    "# Generate the confusion matrices\n",
    "labels = severity_order\n",
    "cm_0 = confusion_matrix(ground_truth, predictions_0, labels=labels)\n",
    "cm_8 = confusion_matrix(ground_truth, predictions_8, labels=labels)\n",
    "\n",
    "print(\"Confusion matrix shape (0 shots):\", cm_0.shape)\n",
    "print(\"Confusion matrix shape (8 shots):\", cm_8.shape)\n",
    "\n",
    "# Define the cost matrix based on the difference in severity indices\n",
    "n_labels = len(labels)\n",
    "cost_matrix = np.zeros((n_labels, n_labels))\n",
    "for i in range(n_labels):\n",
    "    for j in range(n_labels):\n",
    "        cost_matrix[i, j] = abs(i - j)\n",
    "\n",
    "print(\"Cost matrix shape:\", cost_matrix.shape)\n",
    "\n",
    "def calculate_misclassification_cost(cm, cost_matrix):\n",
    "    N = np.sum(cm)  # Total number of observations\n",
    "    cost = np.sum(cm * cost_matrix) / N\n",
    "    return cost\n",
    "\n",
    "# Calculate misclassification costs\n",
    "cost_0 = calculate_misclassification_cost(cm_0, cost_matrix)\n",
    "cost_8 = calculate_misclassification_cost(cm_8, cost_matrix)\n",
    "\n",
    "print(f\"Misclassification cost (0 shots): {cost_0:.4f}\")\n",
    "print(f\"Misclassification cost (8 shots): {cost_8:.4f}\")\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot the first confusion matrix\n",
    "disp_0 = ConfusionMatrixDisplay(confusion_matrix=cm_0, display_labels=labels)\n",
    "disp_0.plot(cmap=plt.cm.Blues, ax=axs[0], xticks_rotation='vertical')\n",
    "axs[0].set_title(f'Confusion Matrix (0 shots)\\nMisclassification cost: {cost_0:.4f}')\n",
    "\n",
    "# Plot the second confusion matrix\n",
    "disp_8 = ConfusionMatrixDisplay(confusion_matrix=cm_8, display_labels=labels)\n",
    "disp_8.plot(cmap=plt.cm.Blues, ax=axs[1], xticks_rotation='vertical')\n",
    "axs[1].set_title(f'Confusion Matrix (8 shots)\\nMisclassification cost: {cost_8:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "data_path = \"YELLOW-RUST-19.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Define the order of severity for the labels\n",
    "severity_order = [\n",
    "    'No disease (0)',\n",
    "    'Resistant (R)',\n",
    "    'Moderately Resistant (MR)',\n",
    "    'MRMS',\n",
    "    'Moderately Susceptible (MS)',\n",
    "    'Susceptible (S)'\n",
    "]\n",
    "\n",
    "# Create a mapping from labels to severity indices\n",
    "label_to_index = {label: index for index, label in enumerate(severity_order)}\n",
    "\n",
    "# Convert ground truth to numeric values\n",
    "y_true = data['1'].map(label_to_index)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Remove rows with NaN values\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true_clean, y_pred_clean)\n",
    "    f1 = f1_score(y_true_clean, y_pred_clean, average='weighted')\n",
    "    kappa = cohen_kappa_score(y_true_clean, y_pred_clean, weights='quadratic')\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    mse = mean_squared_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return accuracy, f1, kappa, mae, rmse\n",
    "\n",
    "# Calculate metrics for each shot\n",
    "shots = [0, 1, 2, 4, 8]\n",
    "metrics = []\n",
    "\n",
    "for shot in shots:\n",
    "    y_pred = data[f'# of Shots {shot}'].map(label_to_index)\n",
    "    metrics.append(calculate_metrics(y_true, y_pred))\n",
    "\n",
    "# Unpack metrics\n",
    "accuracies, f1_scores, kappas, maes, rmses = zip(*metrics)\n",
    "\n",
    "# Create two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
    "\n",
    "# Plot 1: Accuracy-based metrics\n",
    "ax1.plot(shots, accuracies, marker='o', label='Accuracy')\n",
    "ax1.plot(shots, f1_scores, marker='s', label='F1 Score')\n",
    "ax1.plot(shots, kappas, marker='^', label='Quadratic Weighted Kappa')\n",
    "\n",
    "ax1.set_title('Accuracy-based Metrics for Yellow Rust Dataset')\n",
    "ax1.set_xlabel('Number of Shots')\n",
    "ax1.set_ylabel('Metric Value')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.set_xticks(shots)\n",
    "\n",
    "# Add value labels for accuracy-based metrics\n",
    "# for i, shot in enumerate(shots):\n",
    "#     ax1.text(shot, accuracies[i], f'{accuracies[i]:.2f}', ha='center', va='bottom')\n",
    "#     ax1.text(shot, f1_scores[i], f'{f1_scores[i]:.2f}', ha='center', va='bottom')\n",
    "#     ax1.text(shot, kappas[i], f'{kappas[i]:.2f}', ha='center', va='top')\n",
    "\n",
    "# Plot 2: Error-based metrics\n",
    "ax2.plot(shots, maes, marker='D', label='Mean Absolute Error')\n",
    "ax2.plot(shots, rmses, marker='*', label='Root Mean Squared Error')\n",
    "\n",
    "ax2.set_title('Error-based Metrics for Yellow Rust Dataset')\n",
    "ax2.set_xlabel('Number of Shots')\n",
    "ax2.set_ylabel('Metric Value')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.set_xticks(shots)\n",
    "\n",
    "# Add value labels for error-based metrics\n",
    "# for i, shot in enumerate(shots):\n",
    "#     ax2.text(shot, maes[i], f'{maes[i]:.2f}', ha='center', va='bottom')\n",
    "#     ax2.text(shot, rmses[i], f'{rmses[i]:.2f}', ha='center', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Metrics for each shot:\")\n",
    "for i, shot in enumerate(shots):\n",
    "    print(f\"\\nShot {shot}:\")\n",
    "    print(f\"Accuracy: {accuracies[i]:.4f}\")\n",
    "    print(f\"F1 Score: {f1_scores[i]:.4f}\")\n",
    "    print(f\"Quadratic Weighted Kappa: {kappas[i]:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {maes[i]:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def plot_f1_scores(file_names):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        # Load the data\n",
    "        data = pd.read_csv(file_name)\n",
    "        \n",
    "        # Extract the ground truth column\n",
    "        ground_truth = data['1'].astype(str)\n",
    "        \n",
    "        # Get the unique classes present in the ground truth\n",
    "        classes = np.unique(ground_truth)\n",
    "        \n",
    "        # Create a dictionary to map ground truth classes to their indices\n",
    "        class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "        \n",
    "        # Initialize a list to store average F1 scores for each \"# of Shots\" column\n",
    "        avg_f1_scores = []\n",
    "        \n",
    "        # Iterate over the \"# of Shots\" columns\n",
    "        for i in [0, 1, 2, 4, 8]:\n",
    "            # Extract the prediction column\n",
    "            predictions = data[f'# of Shots {i}'].astype(str)\n",
    "            \n",
    "            # Map predictions to their corresponding class indices\n",
    "            mapped_predictions = [class_map.get(pred, -1) for pred in predictions]\n",
    "            \n",
    "            # Get the indices where mapped_predictions is not equal to -1\n",
    "            valid_indices = np.where(np.array(mapped_predictions) != -1)[0]\n",
    "            \n",
    "            # Remove predictions that are not in the class map\n",
    "            mapped_predictions = [mapped_predictions[i] for i in valid_indices]\n",
    "            ground_truth_filtered = ground_truth[valid_indices]\n",
    "            \n",
    "            # Create a reverse dictionary to map indices back to class labels\n",
    "            reverse_class_map = {i: cls for i, cls in enumerate(classes)}\n",
    "            \n",
    "            # Convert mapped_predictions back to class labels\n",
    "            mapped_predictions_labels = [reverse_class_map[pred] for pred in mapped_predictions]\n",
    "            \n",
    "            # Calculate precision, recall, and F1-score\n",
    "            _, _, f1, _ = precision_recall_fscore_support(ground_truth_filtered, mapped_predictions_labels, labels=classes, average='macro')\n",
    "            \n",
    "            # Append the average F1 score to the list\n",
    "            avg_f1_scores.append(f1)\n",
    "        \n",
    "        # Plot average F1 scores for the dataset\n",
    "        plt.plot([0, 1, 2, 4, 8], avg_f1_scores, marker='o', label=file_name)\n",
    "    \n",
    "    # Labeling and aesthetics\n",
    "    plt.xlabel('Number of Shots')\n",
    "    plt.ylabel('Average F1 Score')\n",
    "    plt.title('Average F1 Score vs Number of Shots for Different Datasets')\n",
    "    plt.xticks([0, 1, 2, 4, 8])\n",
    "    plt.ylim(0.2, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Datasets', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"analysis/shots vs overall f1.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "file_names = [ \"10-Durum.csv\",\"Soybean Seeds.csv\", \"mango-leaf-disease-dataset.csv\"  ,\"11-Bean-Leaf.csv\", \"12-deepweeds.csv\"]\n",
    "plot_f1_scores(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt4o-env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
