{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapping = {\n",
    "    'Durum Wheat': ('Identification (I)', 'F1', 'Seed Morphology'),\n",
    "    'Soybean Seeds': ('Identification (I)', 'F1', 'Seed Morphology'),\n",
    "    'Mango Leaf Disease': ('Identification (I)', 'F1', 'Foliar Stress'),\n",
    "    'Bean Leaf Lesions': ('Identification (I)', 'F1', 'Foliar Stress'),\n",
    "    'Soybean Diseases': ('Identification (I)', 'F1', 'Foliar Stress'),\n",
    "    'Dangerous Insects': ('Identification (I)', 'F1', 'Invasive Species'),\n",
    "    'DeepWeeds': ('Identification (I)', 'F1', 'Invasive Species'),\n",
    "    'Yellow Rust 19': ('Classification (C)', 'NMAE', 'Disease Severity'),\n",
    "    'IDC': ('Classification (C)', 'NMAE', 'Stress Tolerance'),\n",
    "    'FUSARIUM 22': ('Classification (C)', 'NMAE', 'Stress Tolerance'),\n",
    "    'InsectCount': ('Quantification (Q)', 'NMAE', 'Pest'),\n",
    "    'PlantDoc': ('Quantification (Q)', 'NMAE', 'Disease'),\n",
    "}\n",
    "\n",
    "\n",
    "#dataset name, question\n",
    "question_mapping = {\n",
    "    'Durum Wheat':'What wheat variety is this?',\n",
    "    'Soybean Seeds':'What soybean lifecycle stage is this?' ,\n",
    "    'Mango Leaf Disease': 'What mango leaf disease is present?' ,\n",
    "    'Bean Leaf Lesions': 'What type of bean leaf lesion is this?',\n",
    "    'Soybean Diseases': 'What is the type of stress in this soybean?',\n",
    "    'Dangerous Insects': 'What is the name of this harmful insect?',\n",
    "    'DeepWeeds':'What is the name of this weed?' ,\n",
    "    'Yellow Rust 19':'What is the severity of yellow rust disease?' ,\n",
    "    'IDC':'What is the rating (1-5) of soybean stress severity?' ,\n",
    "    'FUSARIUM 22':'What is the severity of chickpea fusarium wilt?' ,\n",
    "    'InsectCount': 'What is the insect count?',\n",
    "    'PlantDoc': 'What is the diseased leaf percentage?',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_sample_predictions(n_samples=5):\n",
    "    results = {}\n",
    "    datasets = list(dataset_mapping.keys())\n",
    "    llms = [d for d in os.listdir('results') if os.path.isdir(os.path.join('results', d))]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # Load data for all LLMs\n",
    "        llm_dfs = {llm: pd.read_csv(f'results/{llm}/{dataset}.csv') for llm in llms}\n",
    "        \n",
    "        # Find common filepaths across all LLMs\n",
    "        common_filepaths = set(llm_dfs[llms[0]]['0'])\n",
    "        for llm_df in llm_dfs.values():\n",
    "            common_filepaths &= set(llm_df['0'])\n",
    "        \n",
    "        # Filter dataframes to include only common filepaths\n",
    "        for llm in llms:\n",
    "            llm_dfs[llm] = llm_dfs[llm][llm_dfs[llm]['0'].isin(common_filepaths)]\n",
    "        \n",
    "        # Sample from common filepaths\n",
    "        sampled_filepaths = random.sample(list(common_filepaths), min(n_samples, len(common_filepaths)))\n",
    "        \n",
    "        # Create DataFrame for this dataset\n",
    "        df_data = []\n",
    "        for filepath in sampled_filepaths:\n",
    "            row_data = {'Filepath': filepath}\n",
    "            # Add ground truth from the first LLM (it's the same for all)\n",
    "            row_data['Ground_Truth'] = llm_dfs[llms[0]][llm_dfs[llms[0]]['0'] == filepath]['1'].iloc[0]\n",
    "            for llm, llm_df in llm_dfs.items():\n",
    "                row = llm_df[llm_df['0'] == filepath].iloc[0]\n",
    "                row_data.update({\n",
    "                    f'{llm}_0-shot': row['# of Shots 0'],\n",
    "                    f'{llm}_8-shot': row['# of Shots 8']\n",
    "                })\n",
    "            df_data.append(row_data)\n",
    "        \n",
    "        results[dataset] = pd.DataFrame(df_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Get sample predictions\n",
    "sample_predictions = get_sample_predictions()\n",
    "\n",
    "# Print the results\n",
    "for dataset, df in sample_predictions.items():\n",
    "    print(f\"\\n\\n=== Dataset: {dataset} ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_output(sample_predictions, question_mapping, dataset_mapping, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    latex_outputs = []\n",
    "    files_to_upload = []\n",
    "    \n",
    "    for dataset, df in sample_predictions.items():\n",
    "        # Get dataset information from the mapping\n",
    "        category, metric, subcategory = dataset_mapping[dataset]\n",
    "        \n",
    "        # Get the question for this dataset\n",
    "        question = question_mapping[dataset]\n",
    "        \n",
    "        # Group by ground truth and select one sample from each group\n",
    "        grouped = df.groupby('Ground_Truth')\n",
    "        unique_ground_truths = list(grouped.groups.keys())\n",
    "        \n",
    "        if len(unique_ground_truths) < 2:\n",
    "            print(f\"Warning: Dataset {dataset} has less than 2 unique ground truth categories.\")\n",
    "            continue\n",
    "        \n",
    "        selected_ground_truths = random.sample(unique_ground_truths, 2)\n",
    "        \n",
    "        for ground_truth in selected_ground_truths:\n",
    "            group_df = grouped.get_group(ground_truth)\n",
    "            sample = group_df.iloc[random.randint(0, len(group_df) - 1)]\n",
    "            \n",
    "            # Extract filename from the current path and create new path\n",
    "            current_path = sample['Filepath']\n",
    "            filename = os.path.basename(current_path)\n",
    "            new_path = f\"anecdotal/{dataset}/{filename}\"\n",
    "            \n",
    "            # Add to files to upload\n",
    "            files_to_upload.append((current_path, new_path))\n",
    "            \n",
    "            # Generate the LaTeX output for this sample\n",
    "            latex_output = f\"\"\"\n",
    "            \\\\begin{{figure}}[h]\n",
    "                \\\\centering\n",
    "                \\\\captionsetup{{justification=centering}}\n",
    "                \\\\textbf{{\\\\centering {question}}}\n",
    "                \n",
    "                \\\\vspace{{0.5cm}}\n",
    "                \\\\includegraphics[width=0.2\\\\textwidth]{{{new_path}}}\n",
    "                \\\\caption*{{\n",
    "                    \\\\begin{{tabular}}{{|p{{0.3\\\\linewidth}}|p{{0.3\\\\linewidth}}|p{{0.3\\\\linewidth}}|}}\n",
    "                        \\\\hline\n",
    "                        \\\\textbf{{Category}} & \\\\textbf{{Subcategory}} & \\\\textbf{{Task}} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                        {category} & {subcategory} & {dataset} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                    \\\\end{{tabular}}\n",
    "                    \n",
    "                    \\\\vspace{{0.3cm}}\n",
    "                    \n",
    "                    \\\\textbf{{Ground Truth:}} {sample['Ground_Truth']} \\\\\\\\\n",
    "                    \\\\vspace{{0.2cm}}\n",
    "                    \\\\textbf{{Predictions:}} \\\\\\\\\n",
    "                    \\\\begin{{tabular}}{{l|c|c}}\n",
    "                        \\\\textbf{{Model Name}} & \\\\textbf{{0 shot}} & \\\\textbf{{8 shot}} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                        Gemini-pro-1.5 & {sample['Gemini-pro-1.5_0-shot']} & {sample['Gemini-pro-1.5_8-shot']} \\\\\\\\\n",
    "                        GPT-4o & {sample['GPT-4o_0-shot']} & {sample['GPT-4o_8-shot']} \\\\\\\\\n",
    "                        LLaVA v1.6 34B & {sample['LLaVA v1.6 34B_0-shot']} & {sample['LLaVA v1.6 34B_8-shot']} \\\\\\\\\n",
    "                        Claude-3.5-sonnet & {sample['Claude-3.5-sonnet_0-shot']} & {sample['Claude-3.5-sonnet_8-shot']} \\\\\\\\\n",
    "                        Gemini-flash-1.5 & {sample['Gemini-flash-1.5_0-shot']} & {sample['Gemini-flash-1.5_8-shot']} \\\\\\\\\n",
    "                        Claude-3-haiku & {sample['Claude-3-haiku_0-shot']} & {sample['Claude-3-haiku_8-shot']}\n",
    "                    \\\\end{{tabular}}\n",
    "                }}\n",
    "            \\\\end{{figure}}\n",
    "            \"\"\"\n",
    "            \n",
    "            latex_outputs.append(latex_output)\n",
    "    \n",
    "    return '\\n\\n'.join(latex_outputs), files_to_upload\n",
    "\n",
    "# Assuming question_mapping and dataset_mapping are defined as before\n",
    "\n",
    "# Generate the LaTeX output and get the list of files to upload\n",
    "latex_output, files_to_upload = generate_latex_output(sample_predictions, question_mapping, dataset_mapping)\n",
    "\n",
    "# Print the LaTeX output\n",
    "print(\"LaTeX Output:\")\n",
    "print(latex_output)\n",
    "\n",
    "# Print the list of files to upload\n",
    "print(\"\\nFiles to Upload:\")\n",
    "for original_path, new_path in files_to_upload:\n",
    "    print(f\"Original: {original_path}\")\n",
    "    print(f\"New: {new_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_output(sample_predictions, question_mapping, dataset_mapping, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    latex_outputs = []\n",
    "    files_to_upload = []\n",
    "    \n",
    "    for dataset, df in sample_predictions.items():\n",
    "        # Get dataset information from the mapping\n",
    "        category, metric, subcategory = dataset_mapping[dataset]\n",
    "        \n",
    "        # Get the question for this dataset\n",
    "        question = question_mapping[dataset]\n",
    "        \n",
    "        # Group by ground truth and select one sample from each group\n",
    "        grouped = df.groupby('Ground_Truth')\n",
    "        unique_ground_truths = list(grouped.groups.keys())\n",
    "        \n",
    "        if len(unique_ground_truths) < 2:\n",
    "            print(f\"Warning: Dataset {dataset} has less than 2 unique ground truth categories.\")\n",
    "            continue\n",
    "        \n",
    "        selected_ground_truths = random.sample(unique_ground_truths, 2)\n",
    "        \n",
    "        for ground_truth in selected_ground_truths:\n",
    "            group_df = grouped.get_group(ground_truth)\n",
    "            sample = group_df.iloc[random.randint(0, len(group_df) - 1)]\n",
    "            \n",
    "            # Extract filename from the current path and create new path\n",
    "            current_path = sample['Filepath']\n",
    "            filename = os.path.basename(current_path)\n",
    "            new_path = f\"anecdotal/{dataset}/{filename}\"\n",
    "            \n",
    "            # Add to files to upload\n",
    "            files_to_upload.append((current_path, new_path))\n",
    "            \n",
    "            # Generate the LaTeX output for this sample\n",
    "            latex_output = f\"\"\"\n",
    "            \\\\begin{{figure}}[h]\n",
    "                \\\\centering\n",
    "                \\\\captionsetup{{justification=centering}}\n",
    "                \\\\fontsize{{8pt}}{{9.6pt}}\\\\selectfont  % Set font size to 8pt with 1.2 line spacing\n",
    "                \\\\textbf{{\\\\centering {question}}}\n",
    "                \n",
    "                \\\\vspace{{0.3cm}}\n",
    "                \\\\includegraphics[width=0.45\\\\linewidth]{{{new_path}}}\n",
    "                \\\\caption*{{\n",
    "                    \\\\begin{{tabular}}{{|p{{0.3\\\\linewidth}}|p{{0.3\\\\linewidth}}|p{{0.3\\\\linewidth}}|}}\n",
    "                        \\\\hline\n",
    "                        \\\\textbf{{Category}} & \\\\textbf{{Subcategory}} & \\\\textbf{{Task}} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                        {category} & {subcategory} & {dataset} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                    \\\\end{{tabular}}\n",
    "                    \n",
    "                    \\\\vspace{{0.2cm}}\n",
    "                    \n",
    "                    \\\\textbf{{Ground Truth:}} {sample['Ground_Truth']} \\\\\\\\\n",
    "                    \\\\vspace{{0.1cm}}\n",
    "                    \\\\textbf{{Predictions:}} \\\\\\\\\n",
    "                    \\\\begin{{tabular}}{{l|c|c}}\n",
    "                        \\\\textbf{{Model Name}} & \\\\textbf{{0 shot}} & \\\\textbf{{8 shot}} \\\\\\\\\n",
    "                        \\\\hline\n",
    "                        Gemini-pro-1.5 & {sample['Gemini-pro-1.5_0-shot']} & {sample['Gemini-pro-1.5_8-shot']} \\\\\\\\\n",
    "                        GPT-4o & {sample['GPT-4o_0-shot']} & {sample['GPT-4o_8-shot']} \\\\\\\\\n",
    "                        LLaVA v1.6 34B & {sample['LLaVA v1.6 34B_0-shot']} & {sample['LLaVA v1.6 34B_8-shot']} \\\\\\\\\n",
    "                        Claude-3.5-sonnet & {sample['Claude-3.5-sonnet_0-shot']} & {sample['Claude-3.5-sonnet_8-shot']} \\\\\\\\\n",
    "                        Gemini-flash-1.5 & {sample['Gemini-flash-1.5_0-shot']} & {sample['Gemini-flash-1.5_8-shot']} \\\\\\\\\n",
    "                        Claude-3-haiku & {sample['Claude-3-haiku_0-shot']} & {sample['Claude-3-haiku_8-shot']}\n",
    "                    \\\\end{{tabular}}\n",
    "                }}\n",
    "            \\\\end{{figure}}\n",
    "            \"\"\"\n",
    "            \n",
    "            latex_outputs.append(latex_output)\n",
    "    \n",
    "    return '\\n\\n'.join(latex_outputs), files_to_upload\n",
    "\n",
    "# Assuming question_mapping and dataset_mapping are defined as before\n",
    "\n",
    "# Generate the LaTeX output and get the list of files to upload\n",
    "latex_output, files_to_upload = generate_latex_output(sample_predictions, question_mapping, dataset_mapping)\n",
    "\n",
    "# Print the LaTeX output\n",
    "print(\"LaTeX Output:\")\n",
    "print(latex_output)\n",
    "\n",
    "# Print the list of files to upload\n",
    "print(\"\\nFiles to Upload:\")\n",
    "for original_path, new_path in files_to_upload:\n",
    "    print(f\"Original: {original_path}\")\n",
    "    print(f\"New: {new_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed all final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_output(sample_predictions, question_mapping, dataset_mapping, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    latex_outputs = []\n",
    "    files_to_upload = []\n",
    "    \n",
    "    for dataset, df in sample_predictions.items():\n",
    "        # Get dataset information from the mapping\n",
    "        category, metric, subcategory = dataset_mapping[dataset]\n",
    "        \n",
    "        # Get the question for this dataset\n",
    "        question = question_mapping[dataset]\n",
    "        \n",
    "        # Group by ground truth and select one sample from each group\n",
    "        grouped = df.groupby('Ground_Truth')\n",
    "        unique_ground_truths = list(grouped.groups.keys())\n",
    "        \n",
    "        if len(unique_ground_truths) < 2:\n",
    "            print(f\"Warning: Dataset {dataset} has less than 2 unique ground truth categories.\")\n",
    "            continue\n",
    "        \n",
    "        selected_ground_truths = random.sample(unique_ground_truths, 2)\n",
    "        \n",
    "        for ground_truth in selected_ground_truths:\n",
    "            group_df = grouped.get_group(ground_truth)\n",
    "            sample = group_df.iloc[random.randint(0, len(group_df) - 1)]\n",
    "            \n",
    "            # Extract filename from the current path and create new path\n",
    "            current_path = sample['Filepath']\n",
    "            filename = os.path.basename(current_path)\n",
    "            new_path = f\"anecdotal/{dataset}/{filename}\"\n",
    "            \n",
    "            # Add to files to upload\n",
    "            files_to_upload.append((current_path, new_path))\n",
    "            \n",
    "            # Generate the LaTeX output for this sample\n",
    "            latex_output = f\"\"\"\n",
    "            \\\\begin{{figure}}[t!]\n",
    "                \\\\small\n",
    "                \\\\textbf{{{question}}}\n",
    "                \\\\vspace{{1em}}\n",
    "                \\\\begin{{center}}\n",
    "                \\\\includegraphics[height=0.45\\\\linewidth]{{{new_path}}}\n",
    "                \\\\end{{center}}\n",
    "                \\\\vspace{{1em}}\n",
    "                \n",
    "                \\\\begin{{tabular}}{{|p{{0.28\\\\linewidth}}|p{{0.28\\\\linewidth}}|p{{0.28\\\\linewidth}}|}}\n",
    "                    \\\\hline\n",
    "                    \\\\textbf{{Category}} & \\\\textbf{{Subcategory}} & \\\\textbf{{Task}} \\\\\\\\\n",
    "                    \\\\hline\n",
    "                    {category} & {subcategory} & {dataset} \\\\\\\\\n",
    "                    \\\\hline\n",
    "                \\\\end{{tabular}}\n",
    "                \\\\vspace{{1em}}\n",
    "\n",
    "                \\\\textbf{{Ground Truth:}} {sample['Ground_Truth']}\n",
    "                \\\\vspace{{1em}}\n",
    "\n",
    "                \\\\textbf{{Predictions:}}\n",
    "                \\\\vspace{{1em}}\n",
    "                \n",
    "                \\\\begin{{tabular}}{{p{{0.28\\\\linewidth}}|p{{0.28\\\\linewidth}}|p{{0.28\\\\linewidth}}}}\n",
    "                    \\\\textbf{{Model Name}} & \\\\textbf{{0 shot}} & \\\\textbf{{8 shot}} \\\\\\\\\n",
    "                    \\\\hline\n",
    "                    Gemini-pro-1.5 & {sample['Gemini-pro-1.5_0-shot']} & {sample['Gemini-pro-1.5_8-shot']} \\\\\\\\\n",
    "                    GPT-4o & {sample['GPT-4o_0-shot']} & {sample['GPT-4o_8-shot']} \\\\\\\\\n",
    "                    LLaVA v1.6 34B & {sample['LLaVA v1.6 34B_0-shot']} & {sample['LLaVA v1.6 34B_8-shot']} \\\\\\\\\n",
    "                    Claude-3.5-sonnet & {sample['Claude-3.5-sonnet_0-shot']} & {sample['Claude-3.5-sonnet_8-shot']} \\\\\\\\\n",
    "                    Gemini-flash-1.5 & {sample['Gemini-flash-1.5_0-shot']} & {sample['Gemini-flash-1.5_8-shot']} \\\\\\\\\n",
    "                    Claude-3-haiku & {sample['Claude-3-haiku_0-shot']} & {sample['Claude-3-haiku_8-shot']}\n",
    "                \\\\end{{tabular}}\n",
    "            \\\\end{{figure}}\n",
    "            \"\"\"\n",
    "            \n",
    "            latex_outputs.append(latex_output)\n",
    "    \n",
    "    return '\\n\\n'.join(latex_outputs), files_to_upload\n",
    "\n",
    "# Assuming question_mapping and dataset_mapping are defined as before\n",
    "\n",
    "# Generate the LaTeX output and get the list of files to upload\n",
    "latex_output, files_to_upload = generate_latex_output(sample_predictions, question_mapping, dataset_mapping)\n",
    "\n",
    "# Print the LaTeX output\n",
    "print(\"LaTeX Output:\")\n",
    "print(latex_output)\n",
    "\n",
    "# Print the list of files to upload\n",
    "print(\"\\nFiles to Upload:\")\n",
    "for original_path, new_path in files_to_upload:\n",
    "    print(f\"Original: {original_path}\")\n",
    "    print(f\"New: {new_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt4o-env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
